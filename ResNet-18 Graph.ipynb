{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-18 Architecture Graph Implementation in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "//anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs, Placeholders, and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "x = tf.placeholder(tf.float32, shape=[None,IMAGE_WIDTH,IMAGE_HEIGHT,3]) # represents input 227 x 227 image with 3 color channels (RGB)\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES])\n",
    "hold_prob = tf.placeholder(tf.float32)\n",
    "training = tf.placeholder(tf.bool) # Used for batch normalization - a boolean to indicate whether or not we are training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):  # initializes the weights randomly with a normal distribution\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "def init_bias(shape): # inditializes the bias term as a constant of 0.1 values\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "\n",
    "def max_pool_nbyn(x, name, filter_size=2, stride=2, pad=True):   # creates a max pooling layer\n",
    "    if pad:\n",
    "        return tf.nn.max_pool(x, ksize=[1, filter_size, filter_size, 1],\n",
    "                          strides=[1, stride, stride, 1], padding='SAME', name=name)\n",
    "    else:\n",
    "        return tf.nn.max_pool(x, ksize=[1, filter_size, filter_size, 1],\n",
    "                          strides=[1, stride, stride, 1], padding='VALID', name=name)\n",
    "    \n",
    "def average_pool_nbyn(x, name, filter_size=2, stride=2, pad=True):   # creates a max pooling layer\n",
    "    if pad:\n",
    "        return tf.nn.avg_pool(x, ksize=[1, filter_size, filter_size, 1],\n",
    "                          strides=[1, stride, stride, 1], padding='SAME', name=name)\n",
    "    else:\n",
    "        return tf.nn.avg_pool(x, ksize=[1, filter_size, filter_size, 1],\n",
    "                          strides=[1, stride, stride, 1], padding='VALID', name=name)\n",
    "\n",
    "def global_average_pool(x):\n",
    "    \n",
    "    return tf.reshape(tf.reduce_mean(x, [1,2]), [-1, 1, 1, x.get_shape().as_list()[-1]])\n",
    "\n",
    "\n",
    "def normal_full_layer(input_layer, size):   # creates the fully connected layer\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b  # simple forward propagation using matrix multiplication\n",
    "\n",
    "def batch_normalization(input_layer, training):  # function for batch normalization\n",
    "    \n",
    "    return tf.layers.batch_normalization(input_layer, training=training)\n",
    "\n",
    "def local_response_normalization(input_layer, radius, alpha, beta, name, bias=1.0): # function for local response normalization\n",
    "    \n",
    "     return tf.nn.local_response_normalization(x, depth_radius=radius,\n",
    "                                              alpha=alpha, beta=beta,\n",
    "                                              bias=bias, name=name)\n",
    "\n",
    "def conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name, pad=None, add_relu=True):\n",
    "    \n",
    "    kernel_shape = [filter_height, filter_width]\n",
    "    #init_random_dist = tf.truncated_normal(kernel_shape, stddev=0.1)\n",
    "    if pad is not None:\n",
    "        padded_x = tf.pad(x, [[0, 0], [pad, pad], [pad, pad], [0, 0]])\n",
    "        conv_layer = tf.layers.conv2d(padded_x, filters=num_filters, \n",
    "                            kernel_size=(filter_width, filter_height),\n",
    "                            strides=(stride_x, stride_y),\n",
    "                            kernel_initializer=tf.initializers.truncated_normal(stddev=0.1),\n",
    "                            use_bias=True, bias_initializer=tf.constant_initializer(0.1), name=name)\n",
    "        if add_relu:\n",
    "            return tf.nn.relu(conv_layer)\n",
    "        else:\n",
    "            return conv_layer\n",
    "            \n",
    "    else:\n",
    "        conv_layer = tf.layers.conv2d(x, \n",
    "                            filters=num_filters, \n",
    "                            kernel_size=(filter_width, filter_height),\n",
    "                            strides=(stride_x, stride_y),\n",
    "                            kernel_initializer=tf.initializers.truncated_normal(stddev=0.1), \n",
    "                            use_bias=True, bias_initializer=tf.constant_initializer(0.1), padding=\"same\", name=name)\n",
    "        \n",
    "        if add_relu:\n",
    "            return tf.nn.relu(conv_layer)\n",
    "        else:\n",
    "            return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Generate Simple Two-Layer Residual Units \n",
    "Based on the following diagram from the ResNet paper.\n",
    "<img src=\"resnet_block.png\" style=\"width: 400px; height: 350px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def residual_unit(input_layer, output_channels, name, pad_zero_dim_match=True):\n",
    "    \n",
    "    input_channels = input_layer.get_shape().as_list()[-1] # Gets the number of channels in the input\n",
    "    \n",
    "    if input_channels * 2 == output_channels:\n",
    "        increase_dim = True\n",
    "        stride = 2\n",
    "    elif input_channels == output_channels:\n",
    "        increase_dim = False\n",
    "        stride = 1\n",
    "    else:\n",
    "        raise ValueError('Output and input channel do not match in residual blocks!')\n",
    "    \n",
    "    if increase_dim is True:\n",
    "        if pad_zero_dim_match: # If the we are using the zero-padding approach to match the dimensions\n",
    "            pooled_input = average_pool_nbyn(input_layer, filter_size=2, stride=2, pad=True, name=(name+'_pooled_input'))\n",
    "            padded_input = tf.pad(pooled_input, [[0, 0], [0, 0], [0, 0], [input_channels // 2,\n",
    "                                                                     input_channels // 2]])\n",
    "            conv_1 = conv(input_layer, 3, 3, output_channels, 2, 2, name=(name+'_conv_1'), add_relu=False)\n",
    "            batch_norm_1 = tf.nn.relu(batch_normalization(conv_1, training))\n",
    "            conv_2 = conv(batch_norm_1, 3, 3, output_channels, 1, 1, add_relu=False, name=(name+'_conv_2'))\n",
    "            \n",
    "            return tf.nn.relu(batch_normalization(conv_2, training) + padded_input)\n",
    "        else: # Otherwise the approach for dimension-matching involves 1 x 1 convolutions\n",
    "            conv_1x1 = conv(input_layer, 1, 1, output_channels, 2, 2,name=(name+'_conv_1x1'), add_relu=false)\n",
    "            conv_1x1_batch_norm = tf.nn.relu(batch_normalization(conv_1x1))\n",
    "            \n",
    "            conv_1 = conv(input_layer, 3, 3, output_channels, 2, 2, name=(name+'_conv_1'), add_relu=False)\n",
    "            batch_norm_1 = tf.nn.relu(batch_normalization(conv_1, training))\n",
    "            conv_2 = conv(batch_norm_1, 3, 3, output_channels, 1, 1, add_relu=False, name=(name+'_conv_2'))\n",
    "            \n",
    "            return tf.nn.relu(batch_normalization(conv_2, training) + conv_1x1_batch_norm)\n",
    "        \n",
    "    else:\n",
    "        conv_1 = conv(input_layer, 3, 3, output_channels, 1, 1, name=(name+'_conv_1'))\n",
    "        batch_norm_1 = tf.nn.relu(batch_normalization(conv_1, training))\n",
    "        conv_2 = conv(batch_norm_1, 3, 3, output_channels, 1, 1, add_relu=False, name=(name+'_conv_2'))\n",
    "            \n",
    "        return tf.nn.relu(batch_normalization(conv_2, training) + input_layer)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONV 1: 64 7 x 7 filters with stride = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(112), Dimension(112), Dimension(64)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1 = tf.nn.relu(batch_normalization(conv(x, 7, 7, num_filters=64, stride_x=2, stride_y=2, name='conv_1', add_relu=False), training))\n",
    "conv_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POOL 1: 3 x 3 filter, stride = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(56), Dimension(56), Dimension(64)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_1 = max_pool_nbyn(conv_1, filter_size=3, stride=2, pad=True, name='pool_1')\n",
    "pool_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Unit 1: Two stacked 64-filter 3 x 3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(56), Dimension(56), Dimension(64)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_unit_1 = residual_unit(pool_1, 64, name='res_unit_1')\n",
    "res_unit_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Unit 2: Two stacked 64-filter 3 x 3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(56), Dimension(56), Dimension(64)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_unit_2 = residual_unit(res_unit_1, 64, name='res_unit_2')\n",
    "res_unit_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Unit 3: Two stacked 64-filter 3 x 3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(56), Dimension(56), Dimension(64)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_unit_3 = residual_unit(res_unit_2, 64, name='res_unit_3')\n",
    "res_unit_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Unit 4: Two stacked 128-filter 3 x 3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(128)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_unit_4 = residual_unit(res_unit_3, 128, name='res_unit_4')\n",
    "res_unit_4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Unit 5: Two stacked 128-filter 3 x 3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(128)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_unit_5 = residual_unit(res_unit_4, 128, name='res_unit_5')\n",
    "res_unit_5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Unit 6: Two stacked 128-filter 3 x 3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(128)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_unit_6 = residual_unit(res_unit_5, 128, name='res_unit_6')\n",
    "res_unit_6.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Unit 7: Two stacked 128-filter 3 x 3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(128)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_unit_7 = residual_unit(res_unit_6, 128, name='res_unit_7')\n",
    "res_unit_7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Unit 8: Two stacked 256-filter 3 x 3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(256)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_unit_8 = residual_unit(res_unit_7, 256, name='res_unit_8')\n",
    "res_unit_8.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVG POOL: Global Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1), Dimension(1), Dimension(256)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pool = global_average_pool(res_unit_8)\n",
    "avg_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC : Fully connected layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pool_flattened = tf.reshape(avg_pool, [-1, 256])\n",
    "fc = normal_full_layer(avg_pool_flattened, NUM_CLASSES)\n",
    "fc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = fc\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function with Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-3e2eceb240c6>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001) # Adam optimizer\n",
    "train = optimizer.minimize(cross_entropy)  # training operation\n",
    "\n",
    "init = tf.global_variables_initializer() # global variables initializer"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
